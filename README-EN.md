# UserSphere CLI

ğŸš€ **Intelligent User and Device Management Command Line Tool**

[ä¸­æ–‡æ–‡æ¡£](README.md) | **English Documentation**

UserSphere CLI is an intelligent command-line tool based on RAG (Retrieval-Augmented Generation) technology, using local embedding models for semantic intent matching and supporting natural language interaction for user and device management functions.

## âœ¨ Features

- ğŸ§  **Intelligent Semantic Understanding**: Natural language processing based on local embedding models
- ğŸ” **Efficient Vector Retrieval**: Fast similarity search using FAISS
- ğŸŒ **Multi-language Support**: Support for Chinese, English and other languages
- ğŸ’¾ **Persistent Storage**: Automatic caching of embedding results for faster startup
- ğŸ¯ **Intent Matching**: Precise user intent recognition and action execution
- ğŸ› ï¸ **Modular Design**: Following Google TypeScript Style Guide
- ğŸ”§ **Extensible Architecture**: Support for dynamic addition of new features and language templates

## ğŸ—ï¸ System Architecture

### ğŸ“‹ Project Module Structure Diagram

```mermaid
graph TB
    subgraph "UserSphere CLI Architecture"
        CLI["ğŸ“± CLI Interface<br/>index.ts"]
        RAG["ğŸ¯ RAG System<br/>rag.ts"]
        EMB["ğŸ§  Embedding Service<br/>embedding.ts"]
        VEC["ğŸ“Š Vector Store<br/>vectorStore.ts"]
        ACT["âš¡ Actions<br/>actions.ts"]
        ML["ğŸŒ MultiLang Manager<br/>multilang.ts"]
        PER["ğŸ’¾ Persistence<br/>persistence.ts"]
        
        CLI --> RAG
        RAG --> EMB
        RAG --> VEC
        RAG --> ML
        RAG --> PER
        RAG --> ACT
        
        EMB -.->|"node-llama-cpp"| MODEL["ğŸ¤– Qwen3 Model<br/>1024-dim vectors"]
        VEC -.->|"faiss-node"| FAISS["ğŸ” FAISS Index<br/>L2 Distance"]
        ML -.-> TEMPLATES["ğŸ“ Intent Templates<br/>341 templates, 7 languages"]
        PER -.-> CACHE["ğŸ’¿ Cache Files<br/>embeddings.json<br/>metadata.json"]
        ACT --> BUSINESS["ğŸ’¼ Business Logic<br/>User/Device Management"]
    end
    
    style CLI fill:#e1f5fe
    style RAG fill:#f3e5f5
    style EMB fill:#e8f5e8
    style VEC fill:#fff3e0
    style MODEL fill:#ffebee
    style FAISS fill:#f1f8e9
```

### ğŸ”„ System Workflow Diagram

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant CLI as ğŸ“± CLI Interface
    participant RAG as ğŸ¯ RAG System
    participant ML as ğŸŒ MultiLang
    participant EMB as ğŸ§  Embedding
    participant VEC as ğŸ“Š VectorStore
    participant ACT as âš¡ Actions
    participant CACHE as ğŸ’¾ Cache
    
    Note over U,CACHE: ğŸš€ System Initialization
    CLI->>RAG: Initialize system
    RAG->>EMB: Load Qwen3 model
    EMB-->>RAG: Model ready (1024-dim)
    RAG->>VEC: Initialize FAISS index
    VEC-->>RAG: Index ready
    RAG->>CACHE: Load cached embeddings
    CACHE-->>RAG: 341 intent embeddings
    RAG-->>CLI: System ready
    
    Note over U,CACHE: ğŸ’¬ User Interaction Loop
    U->>CLI: "check my points"
    CLI->>RAG: Process query
    RAG->>ML: Detect language
    ML-->>RAG: Language: en (English)
    RAG->>EMB: Generate embedding
    EMB->>EMB: Call Qwen3 model
    EMB-->>RAG: Query embedding [1024-dim]
    RAG->>VEC: Search similar intents
    VEC->>VEC: FAISS L2 search
    VEC-->>RAG: Best match: getUserPoints (score: 0.07)
    RAG->>ACT: Execute getUserPoints()
    ACT-->>RAG: "Your points: 1280"
    RAG-->>CLI: Formatted response
    CLI-->>U: Display result
    
    Note over U,CACHE: ğŸ’¾ Caching & Optimization
    RAG->>CACHE: Save new embeddings
    CACHE-->>RAG: Cache updated
```

### ğŸ›ï¸ Core Module Details

```
src/
â”œâ”€â”€ ğŸ“± index.ts              # CLI entry point
â”‚   â”œâ”€â”€ Command line parsing
â”‚   â”œâ”€â”€ User interaction interface
â”‚   â””â”€â”€ Main program loop
â”‚
â”œâ”€â”€ ğŸ¯ rag.ts               # RAG core system
â”‚   â”œâ”€â”€ System initialization coordination
â”‚   â”œâ”€â”€ Query processing flow
â”‚   â”œâ”€â”€ Intent matching logic
â”‚   â””â”€â”€ Response generation
â”‚
â”œâ”€â”€ ğŸ§  embedding.ts         # Embedding service
â”‚   â”œâ”€â”€ Qwen3 model loading (node-llama-cpp)
â”‚   â”œâ”€â”€ Text vectorization (1024-dim)
â”‚   â”œâ”€â”€ Batch processing optimization
â”‚   â””â”€â”€ Resource management
â”‚
â”œâ”€â”€ ğŸ“Š vectorStore.ts       # Vector storage
â”‚   â”œâ”€â”€ FAISS index management (faiss-node)
â”‚   â”œâ”€â”€ L2 distance calculation
â”‚   â”œâ”€â”€ Similarity search
â”‚   â””â”€â”€ Batch vector operations
â”‚
â”œâ”€â”€ âš¡ actions.ts           # Business logic
â”‚   â”œâ”€â”€ User management functions
â”‚   â”œâ”€â”€ Device management functions
â”‚   â”œâ”€â”€ System functions
â”‚   â””â”€â”€ Dynamic parameter handling
â”‚
â”œâ”€â”€ ğŸŒ multilang.ts        # Multi-language management
â”‚   â”œâ”€â”€ Language detection
â”‚   â”œâ”€â”€ Intent template management (341 templates)
â”‚   â”œâ”€â”€ 7 language support
â”‚   â””â”€â”€ Dynamic template extension
â”‚
â””â”€â”€ ğŸ’¾ persistence.ts      # Persistent storage
    â”œâ”€â”€ Embedding cache
    â”œâ”€â”€ Metadata management
    â”œâ”€â”€ Cache validation
    â””â”€â”€ Incremental updates
```

### ğŸ› ï¸ Technology Stack Architecture

```mermaid
graph LR
    subgraph "Frontend Layer"
        CLI[ğŸ“± CLI Interface]
        READLINE[ğŸ’¬ Readline]
    end
    
    subgraph "Application Layer"
        RAG[ğŸ¯ RAG Engine]
        ROUTER[ğŸ”€ Intent Router]
        ACTIONS[âš¡ Action Executor]
    end
    
    subgraph "AI/ML Layer"
        EMBEDDING[ğŸ§  Embedding Service]
        VECTOR[ğŸ“Š Vector Search]
        MULTILANG[ğŸŒ Language Detection]
    end
    
    subgraph "Infrastructure Layer"
        LLAMACPP[ğŸ¤– node-llama-cpp]
        FAISS[ğŸ” faiss-node]
        CACHE[ğŸ’¾ File System Cache]
    end
    
    subgraph "Data Layer"
        MODEL["ğŸ“¦ Qwen3 Model<br/>609MB GGUF"]
        TEMPLATES["ğŸ“ Intent Templates<br/>341 items"]
        EMBEDDINGS["ğŸ’¿ Cached Embeddings<br/>10MB JSON"]
    end
    
    CLI --> RAG
    READLINE --> CLI
    RAG --> ROUTER
    ROUTER --> ACTIONS
    RAG --> EMBEDDING
    RAG --> VECTOR
    RAG --> MULTILANG
    
    EMBEDDING --> LLAMACPP
    VECTOR --> FAISS
    MULTILANG --> CACHE
    
    LLAMACPP --> MODEL
    FAISS --> EMBEDDINGS
    MULTILANG --> TEMPLATES
    
    style CLI fill:#e3f2fd
    style RAG fill:#f3e5f5
    style EMBEDDING fill:#e8f5e8
    style LLAMACPP fill:#ffebee
    style MODEL fill:#fce4ec
```

### ğŸ“Š Data Flow Architecture

```mermaid
flowchart TD
    subgraph "Input Processing"
        A["ğŸ‘¤ User Input<br/>check my points"] --> B["ğŸ” Language Detection<br/>English: 90%"]
        B --> C["ğŸ§  Text to Embedding<br/>Qwen3 Model"]
        C --> D["ğŸ“ Vector<br/>1024 dimensions"]
    end
    
    subgraph "Intent Matching"
        D --> E["ğŸ” FAISS Search<br/>L2 Distance"]
        E --> F["ğŸ“Š Similarity Scores<br/>Top 5 matches"]
        F --> G{"ğŸ¯ Score > 0.05?"}
        G -->|Yes| H["âœ… Intent Matched<br/>getUserPoints"]
        G -->|No| I["âŒ No Match<br/>Fallback response"]
    end
    
    subgraph "Action Execution"
        H --> J["âš¡ Execute Action<br/>getUserPoints"]
        J --> K["ğŸ’¼ Business Logic<br/>Fetch user data"]
        K --> L["ğŸ“ Generate Response<br/>Your points: 1280"]
    end
    
    subgraph "Response Generation"
        L --> M["ğŸŒ Localize Response<br/>English format"]
        I --> M
        M --> N["ğŸ“± CLI Output<br/>Formatted display"]
    end
    
    subgraph "Caching Layer"
        C -.->|"Cache hit"| O["ğŸ’¾ Cached Embeddings<br/>341 intents"]
        O -.->|"Load"| E
        L -.->|"Update"| P["ğŸ“ˆ Usage Statistics<br/>Performance metrics"]
    end
    
    style A fill:#e1f5fe
    style D fill:#e8f5e8
    style H fill:#f3e5f5
    style L fill:#fff3e0
    style O fill:#f1f8e9
```

### âš¡ Performance Metrics Architecture

```mermaid
graph TD
    subgraph "Performance Metrics"
        P1["ğŸš€ Startup Performance<br/>Cold: ~3s, Warm: ~0.8s"]
        P2["ğŸ§  Model Loading<br/>Qwen3: ~2.1s, 609MB"]
        P3["ğŸ’¾ Cache Performance<br/>Hit Rate: 95%+"]
        P4["ğŸ” Query Speed<br/>Avg: 150ms, P99: 300ms"]
    end
    
    subgraph "Memory Usage"
        M1["ğŸ“Š Model Memory<br/>~1.2GB RAM"]
        M2["ğŸ’¿ Cache Memory<br/>~50MB RAM"]
        M3["ğŸ”„ Runtime Memory<br/>~200MB RAM"]
        M4["ğŸ“ˆ Total Memory<br/>~1.5GB RAM"]
    end
    
    subgraph "Scalability"
        S1["ğŸ“ Intent Templates<br/>Current: 341<br/>Max: 10,000+"]
        S2["ğŸŒ Languages<br/>Current: 7<br/>Max: 50+"]
        S3["âš¡ Actions<br/>Current: 15<br/>Max: 1,000+"]
        S4["ğŸ” Vector Dimensions<br/>Fixed: 1024"]
    end
    
    P1 --> M1
    P2 --> M2
    P3 --> M3
    P4 --> M4
    
    M1 --> S1
    M2 --> S2
    M3 --> S3
    M4 --> S4
    
    style P1 fill:#e8f5e8
    style M1 fill:#fff3e0
    style S1 fill:#f3e5f5
```

### ğŸ”§ Extension Architecture

```mermaid
graph TB
    subgraph "Extension Points"
        EXT1["â• Add New Language<br/>1. Create templates<br/>2. Generate embeddings<br/>3. Update FAISS"]
        EXT2["âš¡ Add New Action<br/>1. Define function<br/>2. Add templates<br/>3. Register router"]
        EXT3["ğŸ¤– Change Model<br/>1. Download GGUF<br/>2. Update config<br/>3. Rebuild cache"]
        EXT4["ğŸ“Š Custom Metrics<br/>1. Add collectors<br/>2. Export data<br/>3. Dashboard"]
    end
    
    subgraph "Plugin System"
        PLG1["ğŸ”Œ Plugin Interface<br/>IPlugin"]
        PLG2["ğŸ“¦ Plugin Manager<br/>Load/Unload"]
        PLG3["ğŸ”— Plugin Registry<br/>Available plugins"]
        PLG4["âš™ï¸ Plugin Config<br/>Settings & params"]
    end
    
    subgraph "Integration Points"
        INT1["ğŸŒ Web API<br/>REST/GraphQL"]
        INT2["ğŸ“± Mobile SDK<br/>React Native"]
        INT3["ğŸ–¥ï¸ Desktop GUI<br/>Electron"]
        INT4["â˜ï¸ Cloud Deploy<br/>Docker/K8s"]
    end
    
    EXT1 --> PLG1
    EXT2 --> PLG2
    EXT3 --> PLG3
    EXT4 --> PLG4
    
    PLG1 --> INT1
    PLG2 --> INT2
    PLG3 --> INT3
    PLG4 --> INT4
    
    style EXT1 fill:#e1f5fe
    style PLG1 fill:#f3e5f5
    style INT1 fill:#e8f5e8
```

## ğŸ“¦ Installation

### Prerequisites

- Node.js >= 18.0.0
- Memory >= 4GB (for loading embedding models)
- Supported OS: macOS, Linux, Windows

### 1. Clone Repository

```bash
# SSH method (recommended)
git clone git@github.com:HeiSir2014/UserSphere.git
cd UserSphere

# Or use HTTPS method
git clone https://github.com/HeiSir2014/UserSphere.git
cd UserSphere
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Download Embedding Models

Create `models` folder in project root and download models:

```bash
mkdir models
cd models

# Option 1: Qwen3 Embedding Model (Recommended, 1024-dim)
# Source: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/
wget https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf
# Or use curl
curl -L -O https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf

# Option 2: EmbeddingGemma Model (Alternative, 768-dim)
# Source: https://huggingface.co/unsloth/embeddinggemma-300m-GGUF/
wget https://huggingface.co/unsloth/embeddinggemma-300m-GGUF/resolve/main/embeddinggemma-300M-Q8_0.gguf
# Or use curl
curl -L -O https://huggingface.co/unsloth/embeddinggemma-300m-GGUF/resolve/main/embeddinggemma-300M-Q8_0.gguf
```

### 4. Build Project

```bash
npm run build
```

### 5. Start Application

```bash
npm start
# or
./dist/index.js
```

## ğŸš€ Usage

### Basic Commands

```bash
# Start CLI
usersphere

# Specify model file
usersphere --model ./models/qwen3-embedding-0.6b.gguf

# Enable performance statistics
usersphere --timings

# Disable colored output
usersphere --no-color

# Show help
usersphere --help
```

### Interactive Examples

```bash
> check my points
ğŸ¤– Your points: 1280

> what devices do I have?
ğŸ¤– Current bound devices (4):
  â€¢ MacBook-Pro (laptop) - online
  â€¢ iPhone-15 (mobile) - online
  â€¢ iPad-Air (tablet) - offline
  â€¢ iMac-2021 (desktop) - online

> is iPhone online?
ğŸ¤– Device status:
  â€¢ iPhone-15 (mobile) - online - IP: 192.168.1.101

> add device Samsung-Galaxy
ğŸ¤– Device "Samsung-Galaxy" has been successfully added (status: online).

> help
ğŸ¤– UserSphere CLI available functions:
...
```

## ğŸŒ Multi-language Support

UserSphere CLI supports multi-language interaction, the system automatically recognizes language and matches corresponding functions:

### Chinese Examples
```bash
> æŸ¥è¯¢ç§¯åˆ†
> æˆ‘çš„ç”¨æˆ·åæ˜¯ä»€ä¹ˆ
> åˆ—å‡ºæ‰€æœ‰è®¾å¤‡
> iPhoneçŠ¶æ€å¦‚ä½•
```

### English Examples
```bash
> check my points
> what's my username
> list all devices
> iPhone status
```

### Supported Function Categories

| Function Category | Chinese Example | English Example |
|---------|---------|-----------------|
| User Info | æŸ¥è¯¢ç§¯åˆ†ã€ç”¨æˆ·åã€å¤´åƒ | check points, username, avatar |
| Device Management | åˆ—å‡ºè®¾å¤‡ã€è®¾å¤‡çŠ¶æ€ã€æ·»åŠ è®¾å¤‡ | list devices, device status, add device |
| System Functions | å¸®åŠ©ã€ç³»ç»Ÿä¿¡æ¯ã€é€€å‡º | help, system info, exit |

## ğŸ› ï¸ Development

### Project Structure

```
src/
â”œâ”€â”€ embedding.ts     # Embedding service wrapper
â”œâ”€â”€ vectorStore.ts   # Vector storage and retrieval
â”œâ”€â”€ rag.ts          # RAG logic and intent matching
â”œâ”€â”€ actions.ts      # Business logic action implementation
â”œâ”€â”€ multilang.ts    # Multi-language support module
â”œâ”€â”€ persistence.ts  # Persistent storage module
â””â”€â”€ index.ts        # CLI entry point

data/               # Data storage directory
â”œâ”€â”€ embeddings.json # Cached embedding data
â”œâ”€â”€ intents.json    # Intent template configuration
â””â”€â”€ faiss.index     # FAISS index file
```

### Development Commands

```bash
# Run in development mode
npm run dev

# Build project
npm run build

# Code linting
npm run lint

# Fix code style
npm run lint:fix

# Clean build files
npm run clean
```

### Adding New Features

1. **Add new action** (in `actions.ts`):
```typescript
export function newAction(param: string): string {
  return `Execute new feature: ${param}`;
}
```

2. **Add intent template** (in `rag.ts`):
```typescript
{
  text: 'new feature',
  action: 'newAction',
  description: 'Execute new feature',
  category: 'custom',
  examples: ['new feature', 'run new feature'],
}
```

3. **Rebuild and test**:
```bash
npm run build
npm start
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Model file path
USERSPHERE_MODEL_PATH=./models/qwen3-embedding-0.6b.gguf

# Data storage directory
USERSPHERE_DATA_DIR=./data

# Similarity threshold (0.0-1.0)
USERSPHERE_SIMILARITY_THRESHOLD=0.3

# Enable debug mode
USERSPHERE_DEBUG=true
```

### Model Configuration

Supported embedding models:

| Model | Dimensions | Size | Recommended Use |
|------|------|------|----------|
| qwen3-embedding:0.6b | 1024 | ~600MB | General recommendation |
| embeddinggemma | 768 | ~1.2GB | High precision scenarios |

## ğŸ“Š Performance Optimization

### First Startup Optimization

1. **Embedding Cache**: Calculate and cache all intent embeddings on first run
2. **FAISS Index**: Save FAISS index to local file
3. **Fast Loading**: Subsequent startups directly load cached data

### Memory Optimization

- Model loading: ~600MB - 1.2GB
- FAISS index: ~10MB
- Runtime memory: ~200MB

## ğŸ› Troubleshooting

### Common Issues

**Q: Model loading failed**
```bash
âŒ Model file does not exist or is corrupted
ğŸ’¡ Solution: Re-download model file, ensure file integrity
```

**Q: Out of memory**
```bash
âŒ Memory overflow when loading model
ğŸ’¡ Solution: Ensure system has sufficient memory (recommended 4GB+)
```

**Q: Intent recognition inaccurate**
```bash
âŒ System cannot understand user input
ğŸ’¡ Solution: Adjust similarity threshold or add more intent templates
```

### Debug Mode

Enable verbose logging:
```bash
USERSPHERE_DEBUG=true npm start
```

## ğŸ¤ Contributing

1. Fork the project
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push branch: `git push origin feature/amazing-feature`
5. Submit Pull Request

### Code Style

The project follows [Google TypeScript Style Guide](https://google.github.io/styleguide/tsguide.html):

- Use TypeScript strict mode
- Prefer `const` and `readonly`
- Complete type annotations
- Detailed JSDoc comments

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

## ğŸ™ Acknowledgments

- [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) - Local LLM inference
- [faiss-node](https://github.com/ewfian/faiss-node) - Efficient vector retrieval
- [Qwen](https://github.com/QwenLM/Qwen) - Embedding models
- [Google Gemma](https://github.com/google/gemma) - Embedding models

## ğŸ“ Support

- ğŸ“§ Email: heisir21@163.com
- ğŸ› Issues: [GitHub Issues](https://github.com/HeiSir2014/UserSphere/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/HeiSir2014/UserSphere/discussions)

---

<div align="center">

**Made with â¤ï¸ by HeiSir2014**

[â­ Star](https://github.com/HeiSir2014/UserSphere) | [ğŸ´ Fork](https://github.com/HeiSir2014/UserSphere/fork) | [ğŸ“ Report Bug](https://github.com/HeiSir2014/UserSphere/issues)

</div>
